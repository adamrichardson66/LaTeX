\documentclass[11pt,oneside,english]{amsart}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{parskip}
\geometry{verbose,tmargin=0.65in,bmargin=0.65in,lmargin=0.75in,rmargin=0.75in,headheight=0.75cm,headsep=1cm,footskip=1cm}
\setlength{\parskip}{7mm}
\usepackage{setspace}
\onehalfspacing
\pagenumbering{gobble}

\usepackage{bbm}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgffor}
\usetikzlibrary{cd}
\usepackage{ulem}
\usepackage{adjustbox}
\usepackage{bm}
\usepackage{stmaryrd}
\usepackage{cancel}
\usepackage{mathtools}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage[shortlabels]{enumitem}
\setlist[enumerate,1]{label=\textbf{\arabic*.}}
\usepackage{color, colortbl}
\definecolor{Gray}{gray}{0.9}
\usepackage{babel}
\usepackage{mdframed}
\usepackage{esint}
\usepackage[yyyymmdd]{datetime}
\renewcommand{\dateseparator}{--}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}
\hypersetup{urlcolor=blue}





\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem*{proposition*}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem*{lemma}{Lemma}
\newtheorem*{example}{Example}
\newtheorem*{examples}{Examples}
\newtheorem*{definition}{Definition}
\newtheorem*{note}{Nota Bene}

\newcommand{\aspace}{\hspace{7mm}\text{and}\hspace{7mm}}
\newcommand{\ospace}{\hspace{7mm}\text{or}\hspace{7mm}}
\newcommand{\pspace}{\hspace{10mm}}
\newcommand{\lspace}{\vspace{5mm}}
\newcommand{\lhe}{\stackrel{\text{L'H}}{=}}
\newcommand{\lom}[2]{\lim_{{#1}\rightarrow{#2}}}
\newcommand{\ve}{\varepsilon}
\renewcommand{\Re}{\text{Re }}
\renewcommand{\Im}{\text{Im }}
\newcommand{\Log}{\text{Log }}
\newcommand{\ess}{\text{ess sup}}
\newcommand{\dd}[2]{\frac{d{#1}}{d{#2}}}
\newcommand{\pp}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\DD}[2]{\frac{\Delta{#1}}{\Delta{#2}}}
\newcommand{\ovec}[1]{\overrightarrow{#1}}
\newcommand{\MC}[1]{\mathcal{#1}}
\newcommand{\MB}[1]{\mathbb{#1}}
\newcommand{\mbf}[1]{\,\mathbf{#1}}
\renewcommand{\vec}[1]{\underline{#1}}
\newcommand{\Res}{\text{Res}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\p}{\mathbb{P}}
\DeclareMathOperator{\sgn}{sgn}


\def\<#1>{\mathinner{\langle#1\rangle}}

\makeatletter
\g@addto@macro\normalsize{%
  \setlength\belowdisplayshortskip{5mm}
}
\makeatother





\begin{document}

\rightline{Adam D. Richardson}
\rightline{206A - Probability}
\rightline{Cho, Heyrim}
\rightline{HW 2}
\rightline{\today}

\lspace




\begin{enumerate}[leftmargin=*]
\itemsep5mm


\item Let $X_n$ be a sequence of i.i.d random variables and $S_{m,n}=\sum_{i=m+1}^n X_i$. Show that
\[
\left[\p\left(\max_{m<j\leq n}|S_{m,j}|>2a\right)\right]\left[\min_{m<k\leq n}\p(|S_{k,n}|)\leq a\right]\leq \p(|S_{m,n}|>a).
\]

\begin{proof}
Using a hint, for $m< j\leq n$, if $|S_{m,j}|>2a$ and $|S_{j,n}| \leq a$ then $|S_{m,n}|> a$. Let $J$ be the smallest $j>m$ such that $|S_{m,j}|>2a$. Then
\[
\{|S_{m,n}|>a\}\supseteq \bigcup_{m<j\leq n}\{J=j,\,|S_{j,n}|\leq a\}.
\]
The events $\{J=j,\,|S_{j,n}|\leq a\}$ are disjoint since $J$ is a minimum and so is unique, and the events $\{J=j\}$ and $\{|S_{j,n}|\leq a\}$ are independent since they lie in the sigma algebras generated by disjoint disjoint sets of random variables. Thus,
\begin{align*}
\p(|S_{m,n}|>a)&\geq \sum_{j=m+1}^n\p(J=j,\,|S_{j,n}|\leq a)\\[2mm]
&=\sum_{j=m+1}^n\p(J=j)\p(|S_{j,n}|\leq a)\\[2mm]
&\leq \min_{m< k\leq n}\p(|S_{k,n}|\leq a)\sum_{j=m+1}^n\p(J=j)\\[2mm]
&=\left[\min_{m<k\leq n} \p(|S_{k,n}|\leq a)\right]\p(m<J\leq n)\\[2mm]
&=\left[\p\left(\max_{m<j\leq n}|S_{m,j}|>2a\right)\right]\left[\min_{m<k\leq n}\p(|S_{k,n}|)\leq a\right].\qedhere
\end{align*}
\end{proof}


\vfill
\pagebreak
















\item \begin{enumerate}\itemsep5mm \item In general, convergence in distribution does not imply convergence in probability. Consider a sequence of random variables $X_n$ with probability densities $f_{X_n}(x)=(1-\cos(2\pi nx))\1_{(0,1)}(x)$. To which distribution does $X_n$ converge in distribution? Does $X_n$ converge in probability?

Observe that for $x\in[0,1]$,
\begin{align*}
F_{X_n}(x)&=\int_{-\infty}^x(1-\cos(2\pi nt))\1_{(0,1)}(t)\,dt\\[2mm]
&=\int_0^x1-\cos(2\pi nt)\,dt\\[2mm]
&=\left[t-\frac{\sin2\pi nt}{2\pi n}\right]_0^x\\[2mm]
&=x-\frac{\sin2\pi nx}{2\pi n}.
\end{align*}

As $n\to\infty$, the second term goes to 0 since $\sin 2\pi nx$ is bounded and the denominator goes to infinity. Thus, 
\[
\lim_{n\to\infty}F_{X_n}(x)=\begin{cases}0 & \text{if }x<0 \\ x & \text{if } 0\leq x\leq 1 \\ 1 & \text{if }x>1\end{cases}
\]
We claim it does not converge in probability.

\item However, show that if a sequence of random variables $X_n$ converges in distribution to a constant $c$, then it will converge in probability to $c$.

\begin{proof}
Let $\{X_n\}$ be a sequence of random variables, let $\ve>0$ and suppose $X_n \xrightarrow{d} X=c$ where $c$ is a real constant. Then $F_X(x)=0$ if $x<c$ and $F_X(x)=1$ if $x\geq c$, so $F_X$ is continuous everywhere except at $x=c$. By definition we have
\[
\lim_{n\to\infty} F_{X_n}(c-\ve)=0\aspace \lim_{n\to\infty} F_{X_n}(c+\ve)=1.
\]
Then by definition of a cdf we have
\begin{align*}
\lim_{n\to\infty}\p(|X_n-c|\geq \ve)&=\lim_{n\to\infty}\left[\p(X_n\leq c-\ve) + \p(X_n\geq c+\ve)\right]\\[2mm]
&=\lim_{n\to\infty}\p(X_n\leq c-\ve) + \lim_{n\to\infty}\p(X_n\geq c+\ve)\\[2mm]
&=\lim_{n\to\infty}F_{X_n}(c-\ve)+\lim_{n\to\infty}\p(X_n\geq c+\ve)\\[2mm]
&=0+\lim_{n\to\infty}(1-\p(X_n<c+\ve))\\[2mm]
&\leq1-\lim_{n\to\infty}\left(\p(X_n<c+\ve)+\p(X_n=c+\ve)\right)\\[2mm]
&=1-\lim_{n\to\infty}\p(X_n\leq c+\ve)\\[2mm]
&=1-\lim_{n\to\infty}F_{X_n}(c+\ve)\\[2mm]
&=1-1=0.
\end{align*}
Therefore $X_n \xrightarrow{p} c$ by definition.
\end{proof}

\end{enumerate}


\vfill
\pagebreak











\item \begin{enumerate}\itemsep5mm \item If $\varphi(t)$ is a characteristic function, then show that $|\varphi(t)|^2$ is as well.

\begin{proof}
Let $\varphi(t)$ be a characteristic function. Then for independent $X$ and $Y$,
\begin{align*}
|\varphi(t)|^2&=\varphi(t)\overline{\varphi(t)}\\[2mm]
&=\varphi(t)\varphi(-t)\\[2mm]
&=\int_\MB{R} e^{itX}\,d\p\cdot\int_\MB{R}e^{-itY}\,d\p\\[2mm]
&=\int_\MB{R} e^{it(X-Y)}\,d\p\\[2mm]
&=E[e^{it(X-Y)}],
\end{align*}
which is a characteristic function.
\end{proof}


\item If $X_i$ are Poisson random variables with parameter $\lambda_i$, show that $\frac{1}{n}\sum_{i=1}^n X_i$ is a Poisson random variable as well. [Hint: a characteristic function of a Poisson random variable with $\lambda$ is $e^{\lambda(e^{it}-1)}$.]

\begin{proof}
Let $S_n=\sum_{i=1}^nX_i$. First,
\[
\varphi_{X_1+X_2}(t)=\varphi_{X_1}(t)\varphi_{X_2}(t)=\exp(\lambda_1(e^{it}-1))\exp(\lambda_2(e^{it}-1))=\exp((\lambda_1+\lambda_2)(e^{it}-1)).
\]
By induction, the same holds for $S_n=X_1+\cdots+X_n$, and thus
\[
\varphi_{S_n}(t)=\exp\left(\left(\sum_{i=1}^n\lambda_i\right)(e^{it}-1)\right)
\]
Since the characteristic function determines the distribution, we have $S_n$ is a Poisson random variable.
\end{proof}

\end{enumerate}


\vfill\pagebreak

















\item Suppose that $\mu$ is a probability measure and $\varphi(t)=\int e^{itx}\,d\mu(x)$ is its characteristic function. Prove the following inversion formula for $a<b$:
\[
\lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\varphi(t)\,dt=\mu((a,b))+\frac{1}{2}\mu(\{a\})+\frac{1}{2}\mu(\{b\}).
\]
[Hint: Theorem 3.3.11 in Durrett's book.]

\begin{proof}
Following the hint, the proof in Durrett's book, let 
\[
I_T=\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\varphi(t)\,dt=\int_{-T}^T\int\frac{e^{-ita}-e^{-itb}}{it}e^{itx}\mu(dx)\,dt.
\]
Note that 
\[
\left|\frac{e^{-ita}-e^{-itb}}{it}\right|=\left|\frac{1}{it}(e^{-ita}-e^{-itb})\right|=\left|\int_a^be^{-itx}\,dt\right|\leq\int_a^b|e^{-itx}|\,|dt|\leq\int_a^b\,|dt|=|b-a|<\infty
\]
so the integrand above is well-defined for all $t\in\MB{R}$ and we can apply Fubini's theorem. Moreover, using that $\mu$ is a probability measure, $[-T,T]$ is a finite interval, and the parity of the sine and cosine functions, we can decompose the exponential terms into sines and cosines to get
\[
I_T=\int_{-T}^T\int\frac{e^{-ita}-e^{-itb}}{it}e^{itx}\mu(dx)\,dt=\int\left(\int_{-T}^T\frac{\sin(t(x-a))}{t}\,dt-\int_{-T}^T\frac{\sin(t(x-b))}{t}\,dt\right)\mu(dx).
\]
Let $R(\theta,T)=\int_{-T}^T\frac{\sin\theta t}{t}\,dt$. Then we can write
\begin{align*}
I_T&=\int\left(\int_{-T}^T\frac{\sin(t(x-a))}{t}\,dt-\int_{-T}^T\frac{\sin(t(x-b))}{t}\,dt\right)\mu(dx)\\[2mm]
&=\int R(x-a,T)-R(x-b,T)\,\mu(dx).\quad (*)
\end{align*}
Next, let $S(T)=\int_0^T\frac{\sin x}{x}\,dx$. Then for $\theta>0$, using the change of variables $t=\frac{x}{\theta}$ gives
\[
R(\theta,T)=2\int_0^{T\theta}\frac{\sin x}{x}\,dx=2S(T\theta).
\]
If $\theta<0$, then $R(\theta,T)=-R(|\theta|,T)$. Thus $R(\theta,T)=2(\sgn\theta)S(T|\theta|)$ where
\[
\sgn \theta=\begin{cases}1 & \text{if }\theta>0 \\ 0 & \text{if } \theta=0\\ -1 & \text{if }\theta<0.\end{cases}
\]
By Exercise 1.7.5, $S(T)\to\frac{\pi}{2}$ as $T\to\infty$, so $R(\theta,T)\to \pi\sgn\theta$. Thus
\[
R(x-a,T)-R(x-b,T)\to\begin{cases}2\pi & \text{if } a<x<b \\ \pi & \text{if } x=a\text{ or }x=b\\ 0 & \text{if }x<a\text{ or }x>b.
\end{cases}
\]
Moreover, since $S(T)\to\frac{\pi}{2}$, $|S(T)|<\infty$, so $|R(\theta,T)|\leq 2\sup_y S(y)<\infty$. Thus, using the bounded convergence theorem and the result above on $(*)$, we have
\begin{align*}
\lim_{T\to\infty}\frac{1}{2\pi}\int_{-T}^T\frac{e^{-ita}-e^{-itb}}{it}\varphi(t)\,dt&=\lim_{T\to\infty} \frac{1}{2\pi}I_T\\[2mm]
&=\lim_{T\to\infty}\frac{1}{2\pi}\int R(x-a,T)-R(x-b,T)\,\mu(dx)\\[2mm]
&=\frac{1}{2\pi}\int \lim_{T\to\infty} R(x-a,T)-R(x-b,T)\,\mu(dx)\\[2mm]
&=\frac{1}{2\pi}\left(2\pi\mu((a,b))+\pi\mu(\{a\})+\pi\mu(\{b\})\right)\\[2mm]
&=\mu((a,b))+\frac{1}{2}\mu(\{a\})+\frac{1}{2}\mu(\{b\}).\qedhere
\end{align*}
\end{proof}

\end{enumerate}
\end{document}



















